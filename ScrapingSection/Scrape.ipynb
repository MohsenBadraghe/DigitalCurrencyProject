{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cbf5b99f",
      "metadata": {
        "id": "cbf5b99f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86660d53-da8e-48ea-92c6-fc7ea2a51aee",
      "metadata": {
        "id": "86660d53-da8e-48ea-92c6-fc7ea2a51aee"
      },
      "source": [
        "<h1>Finding 200 top coins and creating first table</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f95c55d7-9ede-49fa-9084-722fe0a1c2a0",
      "metadata": {
        "id": "f95c55d7-9ede-49fa-9084-722fe0a1c2a0"
      },
      "outputs": [],
      "source": [
        "# Function to parse HTML & return BeautifulSoup object\n",
        "def HtmlParser(url):\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    page = requests.get(url)\n",
        "    bs = BeautifulSoup(page.content, 'html.parser')\n",
        "    return bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "054a7b85",
      "metadata": {
        "id": "054a7b85"
      },
      "outputs": [],
      "source": [
        "#finding the coins on the specified day by url\n",
        "\n",
        "soup = HtmlParser(\"https://coinmarketcap.com/historical/20230825/\")\n",
        "soup.body.prettify()\n",
        "#all the rows of page\n",
        "AllRows = soup.find_all('tr')\n",
        "MainLinks = []\n",
        "Titles = []\n",
        "for i in range(3 , 203):\n",
        "  cur = AllRows[i].find('a')\n",
        "    #first link allways is coin main link\n",
        "  MainLinks.append(f\"https://coinmarketcap.com{cur['href']}\")\n",
        "    #some coins have title tag and for others which don't the title is written on first content\n",
        "  try:\n",
        "    Titles.append(cur['title'])\n",
        "  except:\n",
        "    Titles.append(cur.contents[0])\n",
        "SymBols = []\n",
        "HystLinks = []\n",
        "for i in range(0 , 200):\n",
        "  url = MainLinks[i]\n",
        "  bs = HtmlParser(url)\n",
        "    #finding symbol based on html pattern\n",
        "  SymBols.append(bs.find('span' , {'data-role':\"coin-symbol\"}).contents[0])\n",
        "  txt = str(bs)\n",
        "  pos = txt.find(\"See historical\")\n",
        "    #search for historical data on the entire content of the page\n",
        "  LinkTxt = txt[pos-200:pos]\n",
        "  pos2 = LinkTxt.find(\"href\")\n",
        "    #the link attached to \"historical data is written somewhere before\n",
        "    #200 chars is enough and 100 chars is not for example\n",
        "  Hlink = LinkTxt[pos2 + 6:]\n",
        "  Hlink = Hlink[ : Hlink.find(\" \") - 1]\n",
        "    #historical data link starts exactly 6 chars after href and ends after \" and an space\n",
        "  HystLinks.append( f\"https://coinmarketcap.com{Hlink}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "88271f00-33df-48f9-be7a-4b2914c3ca7e",
      "metadata": {
        "id": "88271f00-33df-48f9-be7a-4b2914c3ca7e"
      },
      "outputs": [],
      "source": [
        "GitHubLinks = []\n",
        "for i in range(0 , 200):\n",
        "  url = MainLinks[i]\n",
        "  bs = HtmlParser(url)\n",
        "  txt = str(bs)\n",
        "   #search for github in page\n",
        "  pos = txt.find(\"github.com\")\n",
        "  txt2 = txt[pos:]\n",
        "    #some links end in ) and some in \"\n",
        "  pos2 = txt2.find('\"')\n",
        "  pos3 = txt2.find(')')\n",
        "  pos2 = min(pos2 , pos3)\n",
        "  gitlink = txt2[:pos2]\n",
        "  GitHubLinks.append(gitlink)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a16fef97-c6c8-4c3d-b0d8-54c5572c9948",
      "metadata": {
        "id": "a16fef97-c6c8-4c3d-b0d8-54c5572c9948"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(list(zip(Titles, SymBols , MainLinks , HystLinks , GitHubLinks )),\n",
        "               columns =['Name', 'Symbol' , 'MainLink' , 'HistoricalLink' , 'GitHubLink'])\n",
        "df.to_csv(\"ScrapedData/FirstTable.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74292152-05fe-4c34-87b8-d64cf4e59978",
      "metadata": {
        "id": "74292152-05fe-4c34-87b8-d64cf4e59978"
      },
      "outputs": [],
      "source": [
        "Tags = []\n",
        "for i in range(0 , 200):\n",
        "  url = MainLinks[i]\n",
        "  bs = HtmlParser(url)\n",
        "    #find tags based on html\n",
        "  temp = bs.find_all('div' , {'class' : \"sc-16891c57-0 sc-b7faf77f-0 jaPKUl\"})\n",
        "  taglist = []\n",
        "    # no tag\n",
        "  if len(temp) < 1:\n",
        "    Tags.append(taglist)\n",
        "    continue\n",
        "  for x in temp[0]:\n",
        "    taglist.append(x.text)\n",
        "    #the last one is not a tag name and it's common in all coins\n",
        "  taglist.pop()\n",
        "  Tags.append(taglist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ff5d85",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create List of Tags with CurrencyId from Scraped Data\n",
        "TagList = []\n",
        "for i in range(len(Tags)):\n",
        "    tags = Tags[i]\n",
        "    for j in range(len(tags)):\n",
        "        TagList.append((i+1,tags[j]))\n",
        "# create Dataframe from TagList & save it as csv file\n",
        "tags = pd.DataFrame(TagList,columns=['CurrencyId','Tag'])\n",
        "tags.to_csv('ScrapedData/Tags.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2175ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Github Links and replace nan values with empty string\n",
        "githubs = pd.read_csv('ScrapedData/FirstTable.csv')['GitHubLink']\n",
        "githubs.fillna('',inplace=True)\n",
        "# define lists to store scraped data\n",
        "curr_lang = []\n",
        "curr_contributors = []\n",
        "# start scraping\n",
        "for i in range(len(githubs)):\n",
        "    if githubs[i] != '':\n",
        "        # Get Main Github url for Currency\n",
        "        url_split = githubs[i].split('/',2)\n",
        "        text = HtmlParser('https://' + url_split[0] + '/' + url_split[1])\n",
        "        # Scrape Contributors\n",
        "        Contributors = []\n",
        "        people_elems = text.findAll('a',class_='member-avatar')\n",
        "        for element in people_elems:\n",
        "            Contributors.append(element['href'])\n",
        "        # Add Contributors's Github to list with CurrencyId\n",
        "        for contributor in Contributors:\n",
        "            user_git = 'https://github.com' + contributor\n",
        "            curr_contributors.append((i+1,user_git))\n",
        "        # Scrape Languages\n",
        "        languages = []\n",
        "        top_lang = text.find('h4',string='Top languages')\n",
        "        if top_lang != None:\n",
        "            top_lang_elems = top_lang.parent()\n",
        "        for elem in top_lang_elems:\n",
        "            lang_elem = elem.find('span',{'itemprop':\"programmingLanguage\"})\n",
        "            if lang_elem != None:\n",
        "                lang = lang_elem.text\n",
        "                if lang not in languages:\n",
        "                    languages.append(lang)\n",
        "        # Add Languages to list with CurrencyId\n",
        "        for lang in languages:\n",
        "           curr_lang.append((i+1,lang)) \n",
        "\n",
        "# Convert Lists To Dataframe\n",
        "CurrencyLanguages = pd.DataFrame(curr_lang,columns=['CurrencyId','Language'])\n",
        "CurrencyContributors = pd.DataFrame(curr_contributors,columns=['CurrencyId','Contributor'])\n",
        "# Save Dataframes as csv\n",
        "CurrencyLanguages.to_csv('ScrapedData/CurrencyLanguages.csv')\n",
        "CurrencyContributors.to_csv('ScrapedData/CurrencyContributors.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d2eda5c-f13b-4545-bf99-3f82bc38013e",
      "metadata": {
        "id": "2d2eda5c-f13b-4545-bf99-3f82bc38013e"
      },
      "source": [
        "<h1>finding last year data for each coin based on crawled historical links</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da149bf",
      "metadata": {
        "id": "7da149bf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "base_url = os.getcwd()\n",
        "prefs = {'download.default_directory' : base_url + '\\\\HistoricalData'}\n",
        "chrome_options.add_experimental_option('prefs', prefs)\n",
        "driver = webdriver.Chrome(chrome_options)\n",
        "driver.maximize_window()\n",
        "wait = WebDriverWait(driver, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b752f3",
      "metadata": {
        "id": "59b752f3"
      },
      "outputs": [],
      "source": [
        "#reading data from pre created csv from previous part dataframe to don't run the previous part every time\n",
        "df = pd.read_csv(\"ScrapedData/FirstTable.csv\")\n",
        "historical_links = df['HistoricalLink']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f7e73f",
      "metadata": {
        "id": "d4f7e73f",
        "outputId": "bc89b0a5-1a66-43e1-bfe6-7e67eae3cf03"
      },
      "outputs": [],
      "source": [
        "for url in historical_links:\n",
        "    driver.get(url)\n",
        "    #click on calender\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div[2]/div[1]/div[2]/div/div/div/div[2]/div/div[1]/div/button[1]'))).click()\n",
        "    #go to selecting year page\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[1]/div[1]'))).click()\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[1]/div[1]'))).click()\n",
        "    #select start year\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/span[6]'))).click()\n",
        "    #select start month\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[1]/div[1]/div[1]/span[8]'))).click()\n",
        "    #select start day\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[2]/div[4]/div[5]'))).click()\n",
        "    #go to selecting year page\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[1]/div[1]'))).click()\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[1]/div[1]'))).click()\n",
        "    #select end year\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/span[8]'))).click()\n",
        "    #select end month\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[1]/div[1]/div[1]/span[8]'))).click()\n",
        "    #select end day\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[1]/div[1]/div/div/div[2]/div[2]/div[4]/div[6]'))).click()\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"tippy-1\"]/div/div[1]/div/div/div[2]/span/button'))).click()\n",
        "    #waiting for the site to load csv and download\n",
        "    wait.until(EC.visibility_of_element_located((By.XPATH, '//tbody/tr[2]')))\n",
        "    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"__next\"]/div[2]/div[1]/div[2]/div/div/div/div[2]/div/div[1]/div/button[2]'))).click()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
